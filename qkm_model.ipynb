{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a desglosar un poco del codigo y vamos a ir pasando lo a pytorch para trabajar mas a detalle:\n",
    "```python\n",
    "\n",
    "qkm_model = QKMClassModel(encoded_size=encoded_size,\n",
    "                            dim_y=dim_y,\n",
    "                            encoder=encoder,\n",
    "                            n_comp=n_comp,\n",
    "                            sigma=0.1)\n",
    "```\n",
    "Función de llamada del QKMclasModel\n",
    "\n",
    "```python\n",
    "def call(self, input):\n",
    "    encoded = self.encoder(input)\n",
    "    rho_x = pure2dm(encoded)\n",
    "    rho_y =self.qkm(rho_x)\n",
    "    probs = dm2discrete(rho_y)\n",
    "    return probs\n",
    "```\n",
    "\n",
    "Funciones adicionales que calculan el kernel:\n",
    "\n",
    "```python\n",
    "self.kernel = RBFKernelLayer(sigma=sigma, \n",
    "                                    dim=encoded_size, \n",
    "                                    trainable=True)\n",
    "self.qkm = QKMLayer(kernel=self.kernel, \n",
    "                                dim_x=encoded_size,\n",
    "                                dim_y=dim_y, \n",
    "                                n_comp=n_comp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pure2dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado puro\n",
      "tf.Tensor(\n",
      "[[1.  0.  0. ]\n",
      " [0.  0.5 0.5]], shape=(2, 3), dtype=float32)\n",
      "Columna de unos\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32)\n",
      "Matriz de Densidad:\n",
      "tf.Tensor(\n",
      "[[[1.  1.  0.  0. ]]\n",
      "\n",
      " [[1.  0.  0.5 0.5]]], shape=(2, 1, 4), dtype=float32)\n",
      "Forma de la Matriz de Densidad: (2, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def pure2dm(psi):\n",
    "    '''\n",
    "    Construct a factorized density matrix to represent a pure state\n",
    "    Arguments:\n",
    "     psi: tensor of shape (bs, d)\n",
    "    Returns:\n",
    "     dm: tensor of shape (bs, 1, d + 1)\n",
    "    '''\n",
    "    ones = tf.ones_like(psi[:, 0:1])\n",
    "    print(\"Columna de unos\")\n",
    "    print(ones)\n",
    "    dm = tf.concat((ones[:, tf.newaxis, :], psi[:, tf.newaxis, :]), axis=2)\n",
    "    return dm\n",
    "\n",
    "# Crear un tensor de ejemplo para representar un estado puro\n",
    "psi = tf.constant([[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]], dtype=tf.float32)\n",
    "print(\"Estado puro\")\n",
    "print(psi)\n",
    "# Llamar a la función pure2dm para obtener la matriz de densidad factorizada\n",
    "dm = pure2dm(psi)\n",
    "\n",
    "# Imprimir la matriz de densidad resultante\n",
    "print(\"Matriz de Densidad:\")\n",
    "print(dm)\n",
    "\n",
    "# Comprobar la forma de la matriz de densidad\n",
    "print(\"Forma de la Matriz de Densidad:\", dm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado Puro:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.5000]])\n",
      "Columna de unos\n",
      "tensor([[1.],\n",
      "        [1.]])\n",
      "Matriz de Densidad:\n",
      "tensor([[[1.0000, 1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.5000, 0.5000]]])\n",
      "Forma de la Matriz de Densidad: torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "# Definir una función para convertir un estado puro en una matriz de densidad factorizada\n",
    "def pure2dm(psi: Tensor)->Tensor:\n",
    "    '''\n",
    "    Construct a factorized density matrix to represent a pure state\n",
    "    Arguments:\n",
    "     psi: tensor of shape (bs, d)\n",
    "    Returns:\n",
    "     dm: tensor of shape (bs, 1, d + 1)\n",
    "    '''\n",
    "    # Crear la columna adicional de solo unos, con el tamaño de la matriz original\n",
    "    ones: Tensor = torch.ones_like(psi[:, 0:1])\n",
    "    print(\"Columna de unos\")\n",
    "    print(ones)\n",
    "    # Concatenar la nueva columna con la matriz original\n",
    "    dm: Tensor = torch.cat((ones.unsqueeze(1), psi.unsqueeze(1)), dim=2)\n",
    "    return dm\n",
    "\n",
    "# Crear un tensor de ejemplo para representar un estado puro\n",
    "psi = torch.tensor([[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]], dtype=torch.float32)\n",
    "print(\"Estado Puro:\")\n",
    "print(psi)\n",
    "\n",
    "# Llamar a la función pure2dm para obtener la matriz de densidad factorizada\n",
    "dm = pure2dm(psi)\n",
    "\n",
    "# Imprimir la matriz de densidad resultante\n",
    "print(\"Matriz de Densidad:\")\n",
    "print(dm)\n",
    "\n",
    "# Comprobar la forma de la matriz de densidad\n",
    "print(\"Forma de la Matriz de Densidad:\", dm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dm2discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de ejemplo\n",
      "tf.Tensor(\n",
      "[[[0.3 8.  0.7]\n",
      "  [0.7 9.  6.2]]], shape=(1, 2, 3), dtype=float32)\n",
      "Pesos\n",
      "tf.Tensor([[0.3 0.7]], shape=(1, 2), dtype=float32)\n",
      "Caracteristicas\n",
      "tf.Tensor(\n",
      "[[[8.  0.7]\n",
      "  [9.  6.2]]], shape=(1, 2, 2), dtype=float32)\n",
      "Normalizacion de los pesos\n",
      "tf.Tensor([[0.3 0.7]], shape=(1, 2), dtype=float32)\n",
      "Normalización\n",
      "tf.Tensor(\n",
      "[[[ 8.030566]\n",
      "  [10.928861]]], shape=(1, 2, 1), dtype=float32)\n",
      "Normalizacion bajo las caracteristicas\n",
      "tf.Tensor(\n",
      "[[[0.99619377 0.08716695]\n",
      "  [0.8235076  0.5673052 ]]], shape=(1, 2, 2), dtype=float32)\n",
      "Vector de Probabilidades:\n",
      "tf.Tensor([[0.77243596 0.22756405]], shape=(1, 2), dtype=float32)\n",
      "Forma del Vector de Probabilidades: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def dm2comp(dm):\n",
    "    '''\n",
    "    Extract vectors and weights from a factorized density matrix representation\n",
    "    Arguments:\n",
    "     dm: tensor of shape (bs, n, d + 1)\n",
    "    Returns:\n",
    "     w: tensor of shape (bs, n)\n",
    "     v: tensor of shape (bs, n, d)\n",
    "    '''\n",
    "    return dm[:, :, 0], dm[:, :, 1:]\n",
    "    \n",
    "def dm2discrete(dm):\n",
    "    '''\n",
    "    Creates a discrete distribution from the components of a density matrix\n",
    "    Arguments:\n",
    "     dm: tensor of shape (bs, n, d + 1)\n",
    "    Returns:\n",
    "     prob: vector of probabilities (bs, d)\n",
    "    '''\n",
    "    w, v = dm2comp(dm)\n",
    "    print(\"Pesos\"); print(w)\n",
    "    print(\"Caracteristicas\"); print(v)\n",
    "    w = w / tf.reduce_sum(w, axis=-1, keepdims=True)\n",
    "    print(\"Normalizacion de los pesos\"); print(w)\n",
    "    norms_v = tf.expand_dims(tf.linalg.norm(v, axis=-1), axis=-1)\n",
    "    print(\"Normalización\"); print(norms_v)\n",
    "    v = v / norms_v\n",
    "    print(\"Normalizacion bajo las caracteristicas\"); print(v)\n",
    "    probs = tf.einsum('...j,...ji->...i', w, v ** 2, optimize=\"optimal\")\n",
    "    return probs\n",
    "\n",
    "# Crear una matriz de densidad de ejemplo\n",
    "dm = tf.constant([[[0.3, 8.0, 0.7], \n",
    "                    [0.7, 9.0, 6.2]]], dtype=tf.float32)\n",
    "print(\"Matriz de ejemplo\")\n",
    "print(dm)\n",
    "# Llamar a la función dm2discrete para obtener el vector de probabilidades\n",
    "probs = dm2discrete(dm)\n",
    "\n",
    "# Imprimir el vector de probabilidades resultante\n",
    "print(\"Vector de Probabilidades:\")\n",
    "print(probs)\n",
    "\n",
    "# Comprobar la forma del vector de probabilidades\n",
    "print(\"Forma del Vector de Probabilidades:\", probs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de ejemplo\n",
      "tensor([[[0.3000, 8.0000, 0.7000],\n",
      "         [0.7000, 9.0000, 6.2000]]])\n",
      "Pesos\n",
      "tensor([[0.3000, 0.7000]]) torch.Size([1, 2])\n",
      "Caracteristicas\n",
      "tensor([[[8.0000, 0.7000],\n",
      "         [9.0000, 6.2000]]])\n",
      "Normalizacion de los pesos\n",
      "tensor([[0.3000, 0.7000]]) torch.Size([1, 2])\n",
      "Normalización\n",
      "tensor([[[ 8.0306],\n",
      "         [10.9289]]])\n",
      "Normalizacion bajo las caracteristicas\n",
      "tensor([[[0.9962, 0.0872],\n",
      "         [0.8235, 0.5673]]])\n",
      "producto: tensor([[[0.2977, 0.0023],\n",
      "         [0.4747, 0.2253]]])\n",
      "Vector de Probabilidades:\n",
      "tensor([0.7724, 0.2276])\n",
      "Forma del Vector de Probabilidades: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "def dm2discrete(dm: tensor)->tensor:\n",
    "    '''\n",
    "    Creates a discrete distribution from the components of a density matrix\n",
    "    Arguments:\n",
    "     dm: tensor of shape (bs, n, d + 1)\n",
    "    Returns:\n",
    "     prob: vector of probabilities (bs, d)\n",
    "    '''\n",
    "    w, v = dm[:, :, 0], dm[:, :, 1:] # Tomamos la primera columna y su sobrante\n",
    "    print(\"Pesos\"); print(w, w.shape)\n",
    "    print(\"Caracteristicas\"); print(v)\n",
    "\n",
    "    w = w / (torch.sum(w,dim=1, keepdim=True)) # Normalizamos los pesos\n",
    "    print(\"Normalizacion de los pesos\"); print(w, w.shape)\n",
    "\n",
    "    norms_v = torch.norm(v,dim=-1, keepdim= True)\n",
    "    print(\"Normalización\"); print(norms_v)\n",
    "\n",
    "    v = v/ norms_v # Normalizamos las caracterisiticas\n",
    "    print(\"Normalizacion bajo las caracteristicas\"); print(v)\n",
    "    # Prodcuto componente a componente en correspondencia con la fila\n",
    "    # Suma de cada columna para tener un solo vector\n",
    "    produt = torch.mul(torch.transpose(w, 0 ,1), v**2); print(\"producto:\",produt)\n",
    "    probs = torch.sum(produt, dim=(0, 1)) \n",
    "\n",
    "    return probs\n",
    "\n",
    "    \n",
    "# Crear una matriz de densidad de ejemplo en PyTorch\n",
    "dm = torch.tensor([[[0.3, 8.0, 0.7], [0.7, 9.0, 6.2]]], dtype=torch.float32)\n",
    "\n",
    "# Imprimir la matriz de ejemplo\n",
    "print(\"Matriz de ejemplo\")\n",
    "print(dm)\n",
    "\n",
    "# Llamar a la función dm2discrete para obtener el vector de probabilidades\n",
    "probs = dm2discrete(dm)\n",
    "\n",
    "# Imprimir el vector de probabilidades resultante\n",
    "print(\"Vector de Probabilidades:\")\n",
    "print(probs)\n",
    "\n",
    "# Comprobar la forma del vector de probabilidades\n",
    "print(\"Forma del Vector de Probabilidades:\", probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qkm =  QKMLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "self.qkm = QKMLayer(kernel=self.kernel, \n",
    "                                dim_x=encoded_size,\n",
    "                                dim_y=dim_y, \n",
    "                                n_comp=n_comp)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(vals):\n",
    "    '''\n",
    "    Calculate the l1 loss for a batch of vectors\n",
    "    Arguments:\n",
    "        vals: tensor with shape (b_size, n)\n",
    "    '''\n",
    "    b_size = tf.cast(tf.shape(vals)[0], dtype=tf.float32)\n",
    "    vals = vals / tf.norm(vals, axis=1)[:, tf.newaxis]\n",
    "    loss = tf.reduce_sum(tf.abs(vals)) / b_size\n",
    "    return loss\n",
    "\n",
    "def call(self, inputs):        \n",
    "    # Weight regularizers\n",
    "    if self.l1_x != 0:\n",
    "        self.add_loss(self.l1_x * l1_loss(self.c_x))\n",
    "    if self.l1_y != 0:\n",
    "        self.add_loss(self.l1_y * l1_loss(self.c_y))\n",
    "    comp_w = tf.abs(self.c_w) + 1e-6\n",
    "    # normalize comp_w to sum to 1\n",
    "    comp_w = comp_w / tf.reduce_sum(comp_w)\n",
    "    in_w = inputs[:, :, 0]  # shape (b, n_comp_in)\n",
    "    in_v = inputs[:, :, 1:] # shape (b, n_comp_in, dim_x)\n",
    "    out_vw = self.kernel(in_v, self.c_x)  # shape (b, n_comp_in, n_comp)\n",
    "    out_w = (tf.expand_dims(tf.expand_dims(comp_w, axis=0), axis=0) *\n",
    "                tf.square(out_vw)) # shape (b, n_comp_in, n_comp)\n",
    "    out_w = tf.maximum(out_w, self.eps) \n",
    "    out_w_sum = tf.reduce_sum(out_w, axis=2) # shape (b, n_comp_in)\n",
    "    out_w = out_w / tf.expand_dims(out_w_sum, axis=2)\n",
    "    out_w = tf.einsum('...i,...ij->...j', in_w, out_w, optimize=\"optimal\")\n",
    "            # shape (b, n_comp)\n",
    "    if self.l1_act != 0:\n",
    "        self.add_loss(self.l1_act * l1_loss(out_w))\n",
    "    out_w = tf.expand_dims(out_w, axis=-1) # shape (b, n_comp, 1)\n",
    "    out_y_shape = tf.shape(out_w) + tf.constant([0, 0, self.dim_y - 1])\n",
    "    out_y = tf.broadcast_to(tf.expand_dims(self.c_y, axis=0), out_y_shape)\n",
    "    out = tf.concat((out_w, out_y), 2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_w = tf.constant([[[1, 2, 3], \n",
    "                       [4, 5, 6]], \n",
    "                       [[7, 8, 9], \n",
    "                        [10, 11, 12]]])\n",
    "in_w = tf.constant([[[13, 14, 15],\n",
    "                      [16, 17, 18]], \n",
    "                      [[19, 20, 21], \n",
    "                       [22, 23, 24]]])\n",
    "\n",
    "in_v= tf.constant([[[25, 26, 27], \n",
    "                    [28, 29, 30]], \n",
    "                    [[31, 32, 33], \n",
    "                     [34, 35, 36]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m in_w \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant([[[\u001b[39m13\u001b[39m, \u001b[39m14\u001b[39m, \u001b[39m15\u001b[39m],\n\u001b[1;32m      6\u001b[0m                       [\u001b[39m16\u001b[39m, \u001b[39m17\u001b[39m, \u001b[39m18\u001b[39m]], \n\u001b[1;32m      7\u001b[0m                       [[\u001b[39m19\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m21\u001b[39m], \n\u001b[1;32m      8\u001b[0m                        [\u001b[39m22\u001b[39m, \u001b[39m23\u001b[39m, \u001b[39m24\u001b[39m]]])\n\u001b[1;32m     10\u001b[0m in_v\u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant([[[\u001b[39m25\u001b[39m, \u001b[39m26\u001b[39m, \u001b[39m27\u001b[39m], \n\u001b[1;32m     11\u001b[0m                     [\u001b[39m28\u001b[39m, \u001b[39m29\u001b[39m, \u001b[39m30\u001b[39m]], \n\u001b[1;32m     12\u001b[0m                     [[\u001b[39m31\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m33\u001b[39m], \n\u001b[1;32m     13\u001b[0m                      [\u001b[39m34\u001b[39m, \u001b[39m35\u001b[39m, \u001b[39m36\u001b[39m]]])\n\u001b[0;32m---> 15\u001b[0m comp_w \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mabs(c_w) \u001b[39m+\u001b[39m \u001b[39m1e-6\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39m# normalize comp_w to sum to 1\u001b[39;00m\n\u001b[1;32m     17\u001b[0m comp_w \u001b[39m=\u001b[39m comp_w \u001b[39m/\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(comp_w)\n",
      "File \u001b[0;32m~/miniconda3/envs/qkm/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/qkm/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "\n",
    "c_w = tf.constant([[[1, 2, 3], \n",
    "                       [4, 5, 6]], \n",
    "                       [[7, 8, 9], \n",
    "                        [10, 11, 12]]])\n",
    "inputs = tf.constant([[[13, 14, 15],\n",
    "                      [16, 17, 18]], \n",
    "                      [[19, 20, 21], \n",
    "                       [22, 23, 24]]])\n",
    "\n",
    "in_v= tf.constant([[[25, 26, 27], \n",
    "                    [28, 29, 30]], \n",
    "                    [[31, 32, 33], \n",
    "                     [34, 35, 36]]])\n",
    "\n",
    "comp_w = tf.abs(c_w) + 1e-6\n",
    "# normalize comp_w to sum to 1\n",
    "comp_w = comp_w / tf.reduce_sum(comp_w)\n",
    "in_w = inputs[:, :, 0]  # shape (b, n_comp_in)\n",
    "in_v = inputs[:, :, 1:] # shape (b, n_comp_in, dim_x)\n",
    "out_vw = self.kernel(in_v, self.c_x)  # shape (b, n_comp_in, n_comp)\n",
    "out_w = (tf.expand_dims(tf.expand_dims(comp_w, axis=0), axis=0) *\n",
    "            tf.square(out_vw)) # shape (b, n_comp_in, n_comp)\n",
    "out_w = tf.maximum(out_w, self.eps) \n",
    "out_w_sum = tf.reduce_sum(out_w, axis=2) # shape (b, n_comp_in)\n",
    "out_w = out_w / tf.expand_dims(out_w_sum, axis=2)\n",
    "out_w = tf.einsum('...i,...ij->...j', in_w, out_w, optimize=\"optimal\")\n",
    "        # shape (b, n_comp)\n",
    "if self.l1_act != 0:\n",
    "    self.add_loss(self.l1_act * l1_loss(out_w))\n",
    "out_w = tf.expand_dims(out_w, axis=-1) # shape (b, n_comp, 1)\n",
    "out_y_shape = tf.shape(out_w) + tf.constant([0, 0, self.dim_y - 1])\n",
    "out_y = tf.broadcast_to(tf.expand_dims(self.c_y, axis=0), out_y_shape)\n",
    "out = tf.concat((out_w, out_y), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "Tensor original:\n",
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [10 11 12]]]\n",
      "\n",
      "Resultado de la suma a lo largo del eje 2:\n",
      "[[ 6 15]\n",
      " [24 33]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Crear un tensor tridimensional ficticio\n",
    "tensor = tf.constant([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "print(tensor.shape)\n",
    "# Calcular la suma a lo largo del tercer eje (axis=2)\n",
    "sum_along_axis2 = tf.reduce_sum(tensor, axis=2)\n",
    "\n",
    "# Imprimir el tensor original y el resultado de la suma\n",
    "print(\"Tensor original:\")\n",
    "print(tensor.numpy())\n",
    "print(\"\\nResultado de la suma a lo largo del eje 2:\")\n",
    "print(sum_along_axis2.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = RBFKernelLayer\n",
    "\n",
    "```python\n",
    "self.kernel = RBFKernelLayer(sigma=sigma, \n",
    "                                         dim=encoded_size, \n",
    "                                         trainable=True)\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Kernel RBF:\n",
      "[[[0.21259221 0.81666887 0.27527922 0.34850094 0.79120505]\n",
      "  [0.05199006 0.41257712 0.4246773  0.12295436 0.3467263 ]\n",
      "  [0.1489237  0.24285115 0.7343257  0.09322954 0.20964548]]\n",
      "\n",
      " [[0.03198252 0.33258015 0.04884354 0.1684303  0.4052699 ]\n",
      "  [0.8193839  0.47304073 0.12634103 0.78048116 0.36857757]\n",
      "  [0.20553519 0.7188861  0.64144784 0.29154104 0.58870536]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define una función para calcular la matriz de kernel RBF\n",
    "class RBFKernelLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, sigma=1.0, min_sigma=1e-6):\n",
    "        super(RBFKernelLayer, self).__init__()\n",
    "        self.sigma = sigma\n",
    "        self.min_sigma = min_sigma\n",
    "\n",
    "    def call(self, A, B):\n",
    "        '''\n",
    "        Input:\n",
    "            A: tensor of shape (bs, n, d)\n",
    "            B: tensor of shape (m, d)\n",
    "        Result:\n",
    "            K: tensor of shape (bs, n, m)\n",
    "        '''\n",
    "        shape_A = tf.shape(A)\n",
    "        shape_B = tf.shape(B)\n",
    "        A_norm = tf.norm(A, axis=-1)[..., tf.newaxis] ** 2\n",
    "        B_norm = tf.norm(B, axis=-1)[tf.newaxis, tf.newaxis, :] ** 2\n",
    "        A_reshaped = tf.reshape(A, [-1, shape_A[2]])\n",
    "        AB = tf.matmul(A_reshaped, B, transpose_b=True) \n",
    "        AB = tf.reshape(AB, [shape_A[0], shape_A[1], shape_B[0]])\n",
    "        dist2 = A_norm + B_norm - 2. * AB\n",
    "        dist2 = tf.clip_by_value(dist2, 0., np.inf)\n",
    "        sigma = tf.clip_by_value(self.sigma, self.min_sigma, np.inf)\n",
    "        K = tf.exp(-dist2 / (2. * sigma ** 2.)) # type: ignore\n",
    "        return K\n",
    "\n",
    "# Ejemplo de datos de entrada A y B\n",
    "A = tf.constant(np.random.rand(2, 3, 4), dtype=tf.float32)  # (bs, n, d)\n",
    "B = tf.constant(np.random.rand(5, 4), dtype=tf.float32)  # (m, d)\n",
    "\n",
    "# Crear una instancia de la capa de kernel RBF\n",
    "rbf_layer = RBFKernelLayer(sigma=0.5, min_sigma=1e-6)\n",
    "\n",
    "# Calcular la matriz de kernel RBF\n",
    "K = rbf_layer(A, B)\n",
    "\n",
    "# Imprimir la matriz de kernel resultante\n",
    "print(\"Matriz de Kernel RBF:\")\n",
    "print(K.numpy())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "clamp() received an invalid combination of arguments - got (float, float, float), but expected one of:\n * (Tensor input, Tensor min, Tensor max, *, Tensor out)\n * (Tensor input, Number min, Number max, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m rbf_layer \u001b[39m=\u001b[39m RBFKernelLayer(sigma\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, min_sigma\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[39m# Calcular la matriz de kernel RBF\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m K \u001b[39m=\u001b[39m rbf_layer(A, B)\n\u001b[1;32m     43\u001b[0m \u001b[39m# Imprimir la matriz de kernel resultante\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMatriz de Kernel RBF:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/qkm/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m, in \u001b[0;36mRBFKernelLayer.forward\u001b[0;34m(self, A, B)\u001b[0m\n\u001b[1;32m     27\u001b[0m dist2 \u001b[39m=\u001b[39m A_norm \u001b[39m+\u001b[39m B_norm \u001b[39m-\u001b[39m \u001b[39m2.\u001b[39m \u001b[39m*\u001b[39m AB\n\u001b[1;32m     28\u001b[0m dist2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(dist2, \u001b[39m0.\u001b[39m, \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> 29\u001b[0m sigma \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_sigma, \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     30\u001b[0m K \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mdist2 \u001b[39m/\u001b[39m (\u001b[39m2.\u001b[39m \u001b[39m*\u001b[39m sigma \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2.\u001b[39m))\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m K\n",
      "\u001b[0;31mTypeError\u001b[0m: clamp() received an invalid combination of arguments - got (float, float, float), but expected one of:\n * (Tensor input, Tensor min, Tensor max, *, Tensor out)\n * (Tensor input, Number min, Number max, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define una clase para calcular la matriz de kernel RBF\n",
    "class RBFKernelLayer(nn.Module):\n",
    "    def __init__(self, sigma=1.0, min_sigma=1e-6):\n",
    "        super(RBFKernelLayer, self).__init__()\n",
    "        self.sigma = sigma\n",
    "        self.min_sigma = min_sigma\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        '''\n",
    "        Input:\n",
    "            A: tensor de forma (bs, n, d)\n",
    "            B: tensor de forma (m, d)\n",
    "        Result:\n",
    "            K: tensor de forma (bs, n, m)\n",
    "        '''\n",
    "        shape_A = A.size()\n",
    "        shape_B = B.size()\n",
    "        A_norm = torch.norm(A, dim=-1).unsqueeze(-1) ** 2\n",
    "        B_norm = torch.norm(B, dim=-1).unsqueeze(0).unsqueeze(0) ** 2\n",
    "        A_reshaped = A.view(-1, shape_A[2])\n",
    "        AB = torch.matmul(A_reshaped, B.t())\n",
    "        AB = AB.view(shape_A[0], shape_A[1], shape_B[0])\n",
    "        dist2 = A_norm + B_norm - 2. * AB\n",
    "        dist2 = torch.clamp(dist2, 0., float('inf'))\n",
    "        sigma = torch.clamp(self.sigma, self.min_sigma, float('inf'))\n",
    "        K = torch.exp(-dist2 / (2. * sigma ** 2.))\n",
    "        return K\n",
    "\n",
    "# Ejemplo de datos de entrada A y B\n",
    "A = torch.tensor(np.random.rand(2, 3, 4), dtype=torch.float32)  # (bs, n, d)\n",
    "B = torch.tensor(np.random.rand(5, 4), dtype=torch.float32)  # (m, d)\n",
    "\n",
    "# Crear una instancia de la capa de kernel RBF\n",
    "rbf_layer = RBFKernelLayer(sigma=0.5, min_sigma=1e-6)\n",
    "\n",
    "# Calcular la matriz de kernel RBF\n",
    "K = rbf_layer(A, B)\n",
    "\n",
    "# Imprimir la matriz de kernel resultante\n",
    "print(\"Matriz de Kernel RBF:\")\n",
    "print(K.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
